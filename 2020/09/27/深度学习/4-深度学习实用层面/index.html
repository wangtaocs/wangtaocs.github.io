<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-W.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-W.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-W.png?v=5.1.4"><link rel="mask-icon" href="/images/favicon-W.png?v=5.1.4" color="#222"><meta name="keywords" content="Hexo, NexT"><meta name="description" content="训练、验证、测试集如果数据集较小，可将所有数据按照3:1:1的比例分配训练集、验证集和测试集；或7:3的比例分配训练集和测试集（无验证集）如果数据集较大百万级别：训练集 : 验证集 : 测试集 &#x3D; 98% : 1% : 1%过百万级别：训练集 : 验证集 : 测试集 &#x3D; 99.5% : 0.25% : 0.25%（或99.5% : 0.4% : 0.1%）偏差、方差知乎回答：Jason GuUnd"><meta property="og:type" content="article"><meta property="og:title" content="4.深度学习实用层面"><meta property="og:url" content="http://www.wangtaocs.top/2020/09/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2/index.html"><meta property="og:site_name" content="王涛"><meta property="og:description" content="训练、验证、测试集如果数据集较小，可将所有数据按照3:1:1的比例分配训练集、验证集和测试集；或7:3的比例分配训练集和测试集（无验证集）如果数据集较大百万级别：训练集 : 验证集 : 测试集 &#x3D; 98% : 1% : 1%过百万级别：训练集 : 验证集 : 测试集 &#x3D; 99.5% : 0.25% : 0.25%（或99.5% : 0.4% : 0.1%）偏差、方差知乎回答：Jason GuUnd"><meta property="og:image" content="https://s1.ax1x.com/2020/09/27/0AM829.png"><meta property="og:image" content="https://s1.ax1x.com/2020/09/28/0E0gBT.jpg"><meta property="og:image" content="https://s1.ax1x.com/2020/09/28/0ViNEd.png"><meta property="og:image" content="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%253D+%5Cfrac%7Bx-min(x)%7D%7Bmax(x)-min(x)%7D"><meta property="og:image" content="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%253D+%5Cfrac%7Bx-mean(x)%7D%7Bmax(x)-min(x)%7D+"><meta property="og:image" content="https://www.zhihu.com/equation?tex=+x%5E%7B%27%7D+%253D+%5Cfrac%7Bx-mean(x)%7D%7B%5Csigma%7D"><meta property="og:image" content="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%253D+%5Cfrac%7Bx%7D%7B%7C%7Cx%7C%7C%7D"><meta property="og:image" content="https://s1.ax1x.com/2020/10/01/0MIpo6.png"><meta property="og:image" content="https://s1.ax1x.com/2020/10/01/0MIAQH.png"><meta property="og:image" content="https://s1.ax1x.com/2020/10/01/0ML34I.png"><meta property="og:image" content="https://s1.ax1x.com/2020/10/01/0MIc01.png"><meta property="og:image" content="http://www.ai-start.com/dl2017/images/4d0c183882a140ecd205f1618243d7f8.png"><meta property="og:image" content="https://s1.ax1x.com/2020/10/02/0Qd4vF.png"><meta property="og:image" content="http://www.ai-start.com/dl2017/images/db9472c81a2cf6bb704dc398ea1cf017.png"><meta property="og:image" content="https://s1.ax1x.com/2020/10/02/0QDszV.png"><meta property="og:image" content="https://s1.ax1x.com/2020/10/02/0QrUl6.png"><meta property="og:image" content="https://s1.ax1x.com/2020/10/02/0Q2ZY4.png"><meta property="article:published_time" content="2020-09-27T13:37:21.000Z"><meta property="article:modified_time" content="2020-10-08T07:21:44.423Z"><meta property="article:author" content="王涛"><meta property="article:tag" content="blog"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://s1.ax1x.com/2020/09/27/0AM829.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://www.wangtaocs.top/2020/09/27/深度学习/4-深度学习实用层面/"><title>4.深度学习实用层面 | 王涛</title><meta name="generator" content="Hexo 4.2.0"></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">王涛</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">独立思考，发现世界。</h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-question-circle"></i><br>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.wangtaocs.top/2020/09/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="王涛"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="王涛"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">4.深度学习实用层面</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-27T21:37:21+08:00">2020-09-27 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">3.3k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">13</span></div></div></header><div class="post-body" itemprop="articleBody"><h1 id="训练、验证、测试集"><a href="#训练、验证、测试集" class="headerlink" title="训练、验证、测试集"></a>训练、验证、测试集</h1><ol><li>如果数据集较小，可将所有数据按照3:1:1的比例分配训练集、验证集和测试集；<br>或7:3的比例分配训练集和测试集（无验证集）</li><li>如果数据集较大<ul><li>百万级别：训练集 : 验证集 : 测试集 = 98% : 1% : 1%</li><li>过百万级别：训练集 : 验证集 : 测试集 = 99.5% : 0.25% : 0.25%（或99.5% : 0.4% : 0.1%）</li></ul></li></ol><h1 id="偏差、方差"><a href="#偏差、方差" class="headerlink" title="偏差、方差"></a>偏差、方差</h1><blockquote><p>知乎回答：<a href="https://www.zhihu.com/people/grindge" target="_blank" rel="noopener">Jason Gu</a><br><a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" target="_blank" rel="noopener">Understanding the Bias-Variance Tradeoff</a></p></blockquote><p><strong>偏差：</strong>描述的是<strong>预测值（估计值）的期望与真实值之间的差距</strong>。偏差越大，越偏离真实数据，如下图第二行所示。</p><p><strong>方差：</strong>描述的是<strong>预测值</strong>的变化范围，离散程度，也就是<strong>离其期望值的距离</strong>。<strong>方差越大，数据的分布越分散</strong>，如下图右列所示。</p><p><img src="https://s1.ax1x.com/2020/09/27/0AM829.png" alt="0AM829.png" style="zoom:50%"></p><p>举例说明：</p><div class="table-container"><table><thead><tr><th style="text-align:center">Train set error</th><th style="text-align:center">1%</th><th style="text-align:center">15%</th><th style="text-align:center">15%</th><th style="text-align:center">0.5%</th></tr></thead><tbody><tr><td style="text-align:center">Dev set error</td><td style="text-align:center">11%</td><td style="text-align:center">16%</td><td style="text-align:center">30%</td><td style="text-align:center">1%</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">高方差</td><td style="text-align:center">高偏差</td><td style="text-align:center">高偏差&amp;高方差</td><td style="text-align:center">低偏差&amp;低方差</td></tr></tbody></table></div><h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>作用：减少过拟合</p><ul><li>L1范数：<script type="math/tex;mode=display">L1=\|w\|_{1}</script></li><li>L2范数（更加常用）：<script type="math/tex;mode=display">L2=\|w\|_{2}^{2}</script></li></ul><p>正则化：</p><ul><li><p>L1正则化：</p><script type="math/tex;mode=display">J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right) + \frac{\lambda}{m}\|\omega\|_{1}</script></li><li><p>L2正则化：</p><script type="math/tex;mode=display">J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right) + \frac{\lambda}{2 m}\|\omega\|_{2}^{2}</script></li></ul><h2 id="在神经网络中实现正则化"><a href="#在神经网络中实现正则化" class="headerlink" title="在神经网络中实现正则化"></a>在神经网络中实现正则化</h2><script type="math/tex;mode=display">(1)J(w^{[1]},b^{[1]},...,w^{[L]},b^{[L]})=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m}\sum_{l=1}^{L}\|W^{[l]}\|_{F}^{2}</script><p>(1)神经网络的成本函数包含$W^{[1]},b^{[1]}$到$W^{[l]},b^{[l]}$所有参数，$L$为神经网络层数。成本函数等于m个训练样本损失函数的总和的平均值，正则项为：</p><script type="math/tex;mode=display">\frac{\lambda}{2 m}\sum_{l=1}^{L}\|W^{[l]}\|_{F}^{2}</script><p>矩阵中所有元素的平方和为：</p><script type="math/tex;mode=display">\|W^{[l]}\|^{2}</script><script type="math/tex;mode=display">(2)\|W^{[l]}\|_{F}^{2}=\sum_{i=1}^{n^{[l-1]}} \sum_{j=1}^{n^{[l]}}(w_{ij})^{2}</script><p>(2)第一个求和符号其值$i$从1到$n^{[l-1]}$，第二个求和符号其值$j$从1到$n^{[l]}$，因为$W$是一个$n^{[l]}×n^{[l-1]}$的多维矩阵，$n^{[l]}$表示$l$层单元的数量，$n^{[l-1]}$表示$l-1$层单元的数量。</p><h2 id="使用范数实现梯度下降"><a href="#使用范数实现梯度下降" class="headerlink" title="使用范数实现梯度下降"></a>使用范数实现梯度下降</h2><p><img src="https://s1.ax1x.com/2020/09/28/0E0gBT.jpg" alt="0E0gBT.jpg"></p><h1 id="Dropout正则化"><a href="#Dropout正则化" class="headerlink" title="Dropout正则化"></a>Dropout正则化</h1><p>随机删除一些神经网络中的神经元及从该神经元进出的连线。</p><p><img src="https://s1.ax1x.com/2020/09/28/0ViNEd.png" alt="0ViNEd.png" style="zoom:50%"></p><blockquote><p>《深度学习入门》</p></blockquote><p><strong>集成学习</strong>：让多个模型单独学习，推理时再取多个模型的输出平均值，可提高神经网络识别精度。<br>集成学习与Dropout有密切的联系，可以将Dropout理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。推理时通过对神经元的输出乘以删除比例，可以取得模型的平均值。可以理解为，Dropout将集成学习的效果（模拟地）通过一个网络实现了。</p><h2 id="实施Dropout"><a href="#实施Dropout" class="headerlink" title="实施Dropout"></a>实施Dropout</h2><h3 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h3><p>对于已经被激活的矩阵A1：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A1 = [[ <span class="number">0.46544685</span>,  <span class="number">0.34576201</span>, <span class="number">-0.00239743</span>,  <span class="number">0.34576201</span>, <span class="number">-0.22172585</span>],</span><br><span class="line">      [ <span class="number">0.57248826</span>,  <span class="number">0.42527883</span>, <span class="number">-0.00294878</span>,  <span class="number">0.42527883</span>, <span class="number">-0.27271738</span>],</span><br><span class="line">      [ <span class="number">0.45465921</span>,  <span class="number">0.3377483</span>,  <span class="number">-0.00234186</span>,  <span class="number">0.3377483</span>,  <span class="number">-0.21658692</span>]]</span><br><span class="line">A1 = np.array(A1)</span><br></pre></td></tr></table></figure><p><code>keep_prob</code>表示保留神经元的概率</p><ol><li><p>初始化一个mask：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mask = np.random.randn(A1.shape[<span class="number">0</span>], A1.shape[<span class="number">1</span>])</span><br><span class="line">mask = mask &lt; keep_prob  <span class="comment"># mask会变成一个布尔型数组</span></span><br></pre></td></tr></table></figure></li><li><p>使用mask对A1进行遮罩：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A1 = np.multiply(A1, mask)</span><br></pre></td></tr></table></figure></li><li><p>为了修正期望值，需要除以<code>keep_prob</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A1 = A1 / keep_prob</span><br></pre></td></tr></table></figure><hr><p><em>这里说一点自己的理解：这个mask翻译成遮罩就非常灵性，可以想象到有一个跟A1一样大的板子叫mask，遮挡在A1上，值为True的地方是个洞，就可以使A1中的数字露出来，而每次这些洞的位置都是随机的。</em></p></li></ol><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>与正向传播非常类似，只需要两步，<strong>正向传播与反向传播需要使用相同的mask</strong>，所以反向传播时，要将正向传播所使用的mask作为参数，传递给反向传播的函数。</p><p>假设只有2层的神经网络：</p><ol><li><p>使用mask对dA2进行遮罩：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dA2 = np.multiply(mask,dA2)</span><br></pre></td></tr></table></figure></li><li><p>修正期望值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dA2 = dA2 / keep_prob</span><br></pre></td></tr></table></figure></li></ol><h1 id="理解dropout"><a href="#理解dropout" class="headerlink" title="理解dropout"></a>理解dropout</h1><p>不依赖于某一个特定的特征，所以必须将权重传播出去。因为神经元可能会被随时删除，通过传播权重，给其他输入增加一点权重，从而达到压缩权重的平方范数的效果，这和L2正则化的效果类似。</p><p>每一层的<strong>keep_prob</strong>可以不同，如果担心某些层比其他层更容易发生过拟合，可以将keep_prob设置地更小；缺点是为了使用交叉验证，需要搜索更多的参数。另一种方案是在一些层上应用<strong>dropout</strong>，而有些层不用dropout，应用dropout的层只含有一个超级参数，就是keep-prob。</p><p>在计算机视觉领域，dropout很常用，因为输入的像素点很多，但要牢记dropout只是一种正则化方法。dropout的缺点是代价函数J不明确，每次迭代都会随机移除一些节点，导致无法确定梯度下降的性能。这导致我们所优化的代价函数失去了其应有的意义。</p><h1 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h1><h2 id="数据扩增"><a href="#数据扩增" class="headerlink" title="数据扩增"></a>数据扩增</h2><p>如果采集新的数据比较困难，可以通过对图片的基本操作：旋转、反转、裁剪等增加训练集，虽然这种方式不如新数据的效果好，但基本没有额外花销。从而以近乎零成本正则化数据集，减少过拟合。</p><h2 id="early-stopping"><a href="#early-stopping" class="headerlink" title="early stopping"></a>early stopping</h2><p>在模型还没有发生过拟合（或者过拟合较低）的情况下，提早结束神经网络的训练过程。</p><p>优点：</p><ul><li>防止过拟合</li><li>模型训练的代价较低</li></ul><p>缺点：（提早结束训练，性能不太好，具体表现如下）</p><ul><li><p>代价函数J没有尽可能降到最低</p></li><li><p>模型的方差可能也没有降到最低</p><hr><p><em>通过一种办法同时降低偏差与方差的方法显然是不太现实的。</em></p></li></ul><p>early stopping适用于对模型要求不高的情况，但我认为深度学习没有最好，只有更好，我们只会追求越来越好的模型，而不是在某一个地方驻足，这显然是一个”治标不治本“的办法。</p><h1 id="归一化输入"><a href="#归一化输入" class="headerlink" title="归一化输入"></a>归一化输入</h1><p>先说说什么是<strong>归一化</strong>：</p><p>让所有数据映射到(0,1)中的方法叫做归一化。公式为：</p><script type="math/tex;mode=display">x^{\prime}=\frac{x-\min (x)}{\max (x)-\min (x)}</script><p>这里的说法其实不太准确，引用知乎答者的回答：</p><blockquote><p><a href="https://www.zhihu.com/people/jaimer-ais" target="_blank" rel="noopener">gokenu</a></p><p>正在做相关的作业,我来做一些小小的努力,不让概念太混乱</p><p>查看了<a href="http://www.zhihu.com/people/bee96a251718343e7e390ecf0a66cc48" target="_blank" rel="noopener">@龚焱</a>的回答中提到的wiki 大致意思是归一化和标准化都属于四种Feature scaling(特征缩放),这四种分别是</p><ol><li>Rescaling (min-max normalization) 有时简称normalization(有点坑)<img src="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%3D+%5Cfrac%7Bx-min%28x%29%7D%7Bmax%28x%29-min%28x%29%7D" alt="[公式]"></li><li>Mean normalization <img src="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%3D+%5Cfrac%7Bx-mean%28x%29%7D%7Bmax%28x%29-min%28x%29%7D+" alt="[公式]"></li><li>Standardization(Z-score normalization) <img src="https://www.zhihu.com/equation?tex=+x%5E%7B%27%7D+%3D+%5Cfrac%7Bx-mean%28x%29%7D%7B%5Csigma%7D" alt="[公式]"></li><li>Scaling to unit length <img src="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%3D+%5Cfrac%7Bx%7D%7B%7C%7Cx%7C%7C%7D" alt="[公式]"></li></ol><p>对比了一下其它回答和一些博客,一般把第一种叫做归一化,第三种叫做标准化.不是很清楚是怎么翻译的.正则化的英文应该是Regularization,有些博客把这也弄混了.正则化是完全不同的事情了.</p><p>然后关于在ML里面是用第一个好还是第三个好,感觉大家都讨论的很激烈.有的认为取决于你的数据的特点(是否稀疏),有的认为取决于数据是否有明确的界限. 个人不太赞同只有归一化让椭圆变成了圆的想法,在我的梯度下降中,两种都加速得挺好…</p></blockquote><hr><p>归一化输入分为两步：</p><ol><li>零均值化</li><li>归一化方差</li></ol><p>使用二维数据说明什么是归一化输入：</p><p><img src="https://s1.ax1x.com/2020/10/01/0MIpo6.png" alt="0MIpo6.png"></p><p>第一步是零均值化：</p><script type="math/tex;mode=display">\mu=\frac{1}{m} \sum_{i=1}^{m} x^{(i)}</script><script type="math/tex;mode=display">x:=x-\mu</script><p>意思是移动训练集，直到它完成零均值化。零均值化后：</p><p><img src="https://s1.ax1x.com/2020/10/01/0MIAQH.png" alt="0MIAQH.png"></p><blockquote><p><a href="http://www.ai-start.com/dl2017/html/lesson2-week1.html" target="_blank" rel="noopener">深度学习笔记</a></p></blockquote><p><img src="https://s1.ax1x.com/2020/10/01/0ML34I.png" alt="0ML34I.png"></p><p>这里应该套用的是</p><script type="math/tex;mode=display">x^{\prime}=\frac{x-\operatorname{mean}(x)}{\sigma}</script><p>只不过经过步骤一，mean变成0了，$\sigma$变成$\sigma^{2}$了</p><p><img src="https://s1.ax1x.com/2020/10/01/0MIc01.png" alt="0MIc01.png"></p><p>以上只是解释了归一化输入的两个步骤，接下来解释为什么要归一化输入。</p><p><img src="http://www.ai-start.com/dl2017/images/4d0c183882a140ecd205f1618243d7f8.png" alt=""></p><p>吴恩达老师的这张ppt非常直观。</p><p>使用归一化输入后，代价函数看起来更加对称，无论从什么位置开始，梯度下降都会更快，可以在梯度下降中使用更大的步长，从而提高学习的速度。</p><h1 id="梯度消失-梯度爆炸"><a href="#梯度消失-梯度爆炸" class="headerlink" title="梯度消失/梯度爆炸"></a>梯度消失/梯度爆炸</h1><p>训练深层次的神经网络时，导数（梯度）会变得非常大或非常小，甚至是指数级变小，这使训练难度变大。</p><p>假设要训练的神经网络长这样：</p><p><img src="https://s1.ax1x.com/2020/10/02/0Qd4vF.png" alt="0Qd4vF.png"></p><p>其中激活函数</p><script type="math/tex;mode=display">g(z)=z,b^{[l]}=0</script><p>则</p><script type="math/tex;mode=display">\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]}\cdot \cdot \cdot W^{[3]}W^{[2]}W^{[1]}x</script><p>又</p><script type="math/tex;mode=display">z^{[1]}=W^{[1]}x,a^{[1]}=g(z^{[1]})=z^{[1]}</script><p>则</p><script type="math/tex;mode=display">\hat{y}=W^{[l]}a^{[l-1]}x</script><p>假设</p><script type="math/tex;mode=display">W^{[l]}=
\begin{bmatrix}
 1.5 & 0\\ 
 0 & 1.5
\end{bmatrix}</script><p>则</p><script type="math/tex;mode=display">\hat{y}=W^{[l]}
\begin{bmatrix}
 1.5 & 0\\ 
 0 & 1.5
\end{bmatrix}^{[l-1]}x</script><p>此时，指数$[l-1]$就会导致梯度爆炸；如果</p><script type="math/tex;mode=display">W^{[l]}=
\begin{bmatrix}
 0.5 & 0\\ 
 0 & 0.5
\end{bmatrix}</script><p>此时，指数$[l-1]$就会导致梯度消失。</p><h2 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h2><p>为了解决梯度消失/梯度爆炸，虽然不能彻底解决，但很有效。</p><p>以单个神经元为例：</p><p><img src="http://www.ai-start.com/dl2017/images/db9472c81a2cf6bb704dc398ea1cf017.png" alt=""></p><p>有4个输入特征，经过$a=g(z)$处理，最后得到$\hat{y}$。稍后讲深度网络时，这些输入表示为$a^{[l]}$，暂时我们用$x$表示。</p><p><img src="https://s1.ax1x.com/2020/10/02/0QDszV.png" alt="0QDszV.png"></p><ul><li>如果使用<strong>Relu</strong>作为激活函数，则用公式$\sqrt{\frac{2}{n^{[l-1]}}}$</li><li>如果使用<strong>tanh</strong>作为激活函数，则用公式$\sqrt{\frac{1}{n^{[l-1]}}}$</li></ul><p>吴恩达老师说：</p><p><img src="https://s1.ax1x.com/2020/10/02/0QrUl6.png" alt="0QrUl6.png"></p><p>意思是作为超参数时，调整的优先级较低。</p><h1 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h1><p>梯度检验是为了保证<strong>backprop</strong>正确实施，其对整个模型的训练没有作用，为了实现梯度检验，需要先了解梯度的数值逼近</p><h2 id="梯度的数值逼近"><a href="#梯度的数值逼近" class="headerlink" title="梯度的数值逼近"></a>梯度的数值逼近</h2><p>这里老师就是讲了个导数的第二种定义（双边误差）：</p><script type="math/tex;mode=display">\left.f^{\prime} (\theta\right)=\frac{f(\theta + \varepsilon)-f(\theta-\varepsilon)}{2 \varepsilon}</script><p>使用双边误差而不使用单边误差是因为更加精确。</p><p>比较好理解，不多赘述了</p><h2 id="梯度检验-1"><a href="#梯度检验-1" class="headerlink" title="梯度检验"></a>梯度检验</h2><p>将所有$W$和$b$转换为向量，做连接运算，从而组合成一个巨大向量$\theta$，代价函数$J$是所有$W$和$b$的函数，即$J$是$\theta$的函数：</p><script type="math/tex;mode=display">J\left(W^{[1]}, b^{[1]}, \ldots, W^{[l]}, b^{[l]}\right)=J(\theta)</script><p>将所有$dW$和$db$转换为矩阵，注意$dW$和$W$、$db$和$b$具有相同的维度。同样地，经过转换与连接操作后，得到一个巨大向量$d\theta$，它与$\theta$具有相同的维度，问题是：<strong>$d\theta$和代价函数$J$的梯度（坡度）有什么关系？</strong></p><p>首先，我们要清楚$J$是超参数$\theta$的一个函数，你也可以将$J$展开为$J\left(\theta_{1}, \theta_{2}, \theta_{3}, \ldots \ldots\right)$，不论超级参数向量$\theta$的维度是多少，为了实施梯度检验，要做的就是循环执行，从而对每个$\theta$也就是对每个组成元素计算$d \theta_{\text {approx }}[i]$的值，我使用双边误差，也就是：</p><script type="math/tex;mode=display">d \theta_{\text {approx }}[i]=\frac{J\left(\theta_{1}, \theta_{2}, \ldots \theta_{i}+\varepsilon, \ldots\right)-J\left(\theta_{1}, \theta_{2}, \ldots \theta_{i}-\varepsilon, \ldots\right)}{2 \varepsilon}</script><p>只对$\theta_{i}$增加$\varepsilon$，其它项保持不变，使用的是双边误差，对另一边做同样的操作，减去$\varepsilon$，其它项全都保持不变。</p><p>$d \theta_{\text {approx }}[i]$应该逼近$d \theta_{\text {approx }}$，$d\theta[i]$是代价函数的偏导数，然后需要对i的每个值都执行这个运算，最后得到两个向量，得到$d\theta$的逼近值$d \theta_{\text {approx }}$，它与$d\theta$具有相同维度，它们两个与$\theta$具有相同维度，要做的就是验证这些向量是否彼此接近。</p><p>如何衡量彼此接近？</p><p>计算以下方程式：</p><script type="math/tex;mode=display">\frac{\| d \theta_{approx}-d \theta \|_{2}}{\left\|d \theta_{approx} \right\|_{2}+\|d \theta\|_{2}}</script><p>上式的值为：</p><ul><li>$10^{-7}$：很好</li><li>$10^{-5}$：注意，可能会有bug</li><li>$10^{-3}$：有bug，需要检查所有$\theta$，看是否有一个具体的$i$值，使得$d \theta_{\text {approx }}[i]$ 与 $d \theta[i]$大不相同，并用它来追踪一些求导计算是否正确</li></ul><h2 id="梯度检验的注意事项"><a href="#梯度检验的注意事项" class="headerlink" title="梯度检验的注意事项"></a>梯度检验的注意事项</h2><ol><li><p>梯度检验仅用于调试，不能用于训练</p></li><li><p>如果算法的梯度检验失败，要检查所有项，尝试找到bug</p></li><li><p>在实施梯度检验时，如果使用正则化，请注意正则项。如果代价函数</p><script type="math/tex;mode=display">J(\theta)=\frac{1}{m} \sum L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m} \sum\left\|W^{[l]}\right\|^{2}</script><p>这就是代价函数$J$的定义，$d\theta$等于与$\theta$相关的$J$函数的梯度，包括这个正则项，记住一定要包括这个正则项。</p></li><li><p>梯度检验不能与dropout一起使用，如有需要，则先将dropout的keep_prob设为1，使用梯度检验检查无误后，再调整keep_prob</p></li><li><p><img src="https://s1.ax1x.com/2020/10/02/0Q2ZY4.png" alt="0Q2ZY4.png"></p></li></ol></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2020/09/22/%E5%85%B6%E4%BB%96/2020%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%A4%A7%E8%B5%9BE%E9%A2%98%E5%A4%8D%E7%9B%98/" rel="next" title="2020研究生数学建模大赛E题复盘"><i class="fa fa-chevron-left"></i> 2020研究生数学建模大赛E题复盘</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2020/10/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/5-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" rel="prev" title="5.优化算法">5.优化算法 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">王涛</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">117</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">6</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">20</span> <span class="site-state-item-name">标签</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#训练、验证、测试集"><span class="nav-number">1.</span> <span class="nav-text">训练、验证、测试集</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#偏差、方差"><span class="nav-number">2.</span> <span class="nav-text">偏差、方差</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#正则化"><span class="nav-number">3.</span> <span class="nav-text">正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#在神经网络中实现正则化"><span class="nav-number">3.1.</span> <span class="nav-text">在神经网络中实现正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用范数实现梯度下降"><span class="nav-number">3.2.</span> <span class="nav-text">使用范数实现梯度下降</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dropout正则化"><span class="nav-number">4.</span> <span class="nav-text">Dropout正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#实施Dropout"><span class="nav-number">4.1.</span> <span class="nav-text">实施Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#正向传播"><span class="nav-number">4.1.1.</span> <span class="nav-text">正向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">4.1.2.</span> <span class="nav-text">反向传播</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#理解dropout"><span class="nav-number">5.</span> <span class="nav-text">理解dropout</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#其他正则化方法"><span class="nav-number">6.</span> <span class="nav-text">其他正则化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据扩增"><span class="nav-number">6.1.</span> <span class="nav-text">数据扩增</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#early-stopping"><span class="nav-number">6.2.</span> <span class="nav-text">early stopping</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#归一化输入"><span class="nav-number">7.</span> <span class="nav-text">归一化输入</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度消失-梯度爆炸"><span class="nav-number">8.</span> <span class="nav-text">梯度消失&#x2F;梯度爆炸</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#权重初始化"><span class="nav-number">8.1.</span> <span class="nav-text">权重初始化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度检验"><span class="nav-number">9.</span> <span class="nav-text">梯度检验</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度的数值逼近"><span class="nav-number">9.1.</span> <span class="nav-text">梯度的数值逼近</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度检验-1"><span class="nav-number">9.2.</span> <span class="nav-text">梯度检验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度检验的注意事项"><span class="nav-number">9.3.</span> <span class="nav-text">梯度检验的注意事项</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">王涛</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">58.8k</span></div><script src="https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js"></script><script>window.mermaid&&mermaid.initialize("")</script></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>